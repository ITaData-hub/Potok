import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from typing import List, Dict, Any, Optional, Callable
from datetime import datetime
import asyncio
from pathlib import Path
import uuid

from app.config.settings import get_settings
from app.utils.logger import setup_logger
from app.utils.exceptions import TrainingException, InsufficientDataException
from app.core.models import StatusNet
from app.core.vocabulary import Vocabulary, LabelEncoder
from app.core.dataset import TaskDataset
from app.services.model_manager import ModelManager
from app.schemas.training import TrainingStatus, TrainingProgress

settings = get_settings()
logger = setup_logger("training_service", settings.LOG_LEVEL)

class TrainingService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –¥–æ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π"""
    
    def __init__(self):
        self.model_manager = ModelManager()
        self.active_trainings: Dict[str, TrainingProgress] = {}
        self.device = settings.DEVICE
    
    async def train_new_model(
        self,
        training_examples: List[Dict[str, Any]],
        epochs: int = 30,
        batch_size: int = 32,
        learning_rate: float = 0.001,
        model_name: Optional[str] = None,
        save_checkpoint: bool = True,
        progress_callback: Optional[Callable] = None
    ) -> Dict[str, Any]:
        """
        –û–±—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏ —Å –Ω—É–ª—è
        
        Args:
            training_examples: –ü—Ä–∏–º–µ—Ä—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            epochs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
            batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
            learning_rate: Learning rate
            model_name: –ò–º—è –º–æ–¥–µ–ª–∏
            save_checkpoint: –°–æ—Ö—Ä–∞–Ω—è—Ç—å —á–µ–∫–ø–æ–∏–Ω—Ç—ã
            progress_callback: Callback –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—É—á–µ–Ω–∏—è
        """
        training_id = str(uuid.uuid4())
        start_time = datetime.utcnow()
        
        if len(training_examples) < 10:
            raise InsufficientDataException(
                "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (–º–∏–Ω–∏–º—É–º 10 –ø—Ä–∏–º–µ—Ä–æ–≤)"
            )
        
        if model_name is None:
            model_name = f"{settings.MODEL_NAME}_{start_time.strftime('%Y%m%d')}"
        
        logger.info(
            f"üöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: {model_name} "
            f"(–ø—Ä–∏–º–µ—Ä–æ–≤: {len(training_examples)}, —ç–ø–æ—Ö: {epochs})"
        )
        
        try:
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            vocab = Vocabulary()
            encoders = self._prepare_encoders(training_examples)
            dataset = self._prepare_dataset(training_examples, vocab, encoders)
            
            # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val
            val_size = max(1, int(len(dataset) * 0.1))
            train_size = len(dataset) - val_size
            
            train_dataset, val_dataset = random_split(
                dataset, [train_size, val_size]
            )
            
            train_loader = DataLoader(
                train_dataset,
                batch_size=batch_size,
                shuffle=True
            )
            
            val_loader = DataLoader(
                val_dataset,
                batch_size=batch_size,
                shuffle=False
            ) if val_size > 0 else None
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
            model = StatusNet(
                vocab_size=vocab.vocab_size,
                embedding_dim=settings.EMBEDDING_DIM,
                hidden_dim=settings.HIDDEN_DIM,
                num_statuses=encoders['status'].num_classes
            ).to(self.device)
            
            optimizer = optim.Adam(model.parameters(), lr=learning_rate)
            criterion = nn.CrossEntropyLoss()
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            progress = TrainingProgress(
                training_id=training_id,
                status=TrainingStatus.IN_PROGRESS,
                current_epoch=0,
                total_epochs=epochs,
                elapsed_time_seconds=0
            )
            self.active_trainings[training_id] = progress
            
            # –û–±—É—á–µ–Ω–∏–µ
            best_loss = float('inf')
            training_history = []
            
            for epoch in range(epochs):
                epoch_start = datetime.utcnow()
                
                # Training loop
                model.train()
                train_loss = 0.0
                
                for batch in train_loader:
                    text = batch['text'].to(self.device)
                    status = batch['status'].to(self.device)
                    
                    optimizer.zero_grad()
                    outputs = model(text)
                    loss = criterion(outputs, status)
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                    optimizer.step()
                    
                    train_loss += loss.item()
                
                avg_train_loss = train_loss / len(train_loader)
                
                # Validation loop
                val_loss = None
                if val_loader:
                    model.eval()
                    val_loss_total = 0.0
                    
                    with torch.no_grad():
                        for batch in val_loader:
                            text = batch['text'].to(self.device)
                            status = batch['status'].to(self.device)
                            outputs = model(text)
                            loss = criterion(outputs, status)
                            val_loss_total += loss.item()
                    
                    val_loss = val_loss_total / len(val_loader)
                
                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
                current_loss = val_loss if val_loss else avg_train_loss
                if current_loss < best_loss:
                    best_loss = current_loss
                
                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
                elapsed = (datetime.utcnow() - start_time).total_seconds()
                progress.current_epoch = epoch + 1
                progress.current_loss = avg_train_loss
                progress.best_loss = best_loss
                progress.elapsed_time_seconds = int(elapsed)
                
                if epoch > 0:
                    avg_epoch_time = elapsed / (epoch + 1)
                    remaining_epochs = epochs - (epoch + 1)
                    progress.estimated_remaining_seconds = int(
                        avg_epoch_time * remaining_epochs
                    )
                
                training_history.append({
                    "epoch": epoch + 1,
                    "train_loss": avg_train_loss,
                    "val_loss": val_loss,
                    "timestamp": datetime.utcnow().isoformat()
                })
                
                if progress_callback:
                    await progress_callback(progress)
                
                val_loss_str = f"{val_loss:.4f}" if val_loss is not None else "N/A"
                logger.info(
                    f"Epoch {epoch + 1}/{epochs} | "
                    f"Train Loss: {avg_train_loss:.4f} | "
                    f"Val Loss: {val_loss_str}"
                )
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
            version = self.model_manager.save_model(
                model=model,
                vocab=vocab,
                encoders=encoders,
                model_name=model_name,
                metadata={
                    "training_id": training_id,
                    "epochs": epochs,
                    "batch_size": batch_size,
                    "learning_rate": learning_rate,
                    "total_examples": len(training_examples),
                    "best_loss": best_loss,
                    "training_history": training_history
                }
            )
            
            # –§–∏–Ω–∞–ª—å–Ω—ã–π —Å—Ç–∞—Ç—É—Å
            progress.status = TrainingStatus.COMPLETED
            duration = (datetime.utcnow() - start_time).total_seconds()
            
            result = {
                "training_id": training_id,
                "status": "completed",
                "model_name": model_name,
                "model_version": version,
                "total_examples": len(training_examples),
                "epochs_completed": epochs,
                "final_loss": best_loss,
                "duration_seconds": int(duration),
                "metrics": {
                    "vocab_size": vocab.vocab_size,
                    "train_samples": train_size,
                    "val_samples": val_size,
                    "best_loss": best_loss
                }
            }
            
            logger.info(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {model_name}/{version}")
            return result
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏: {e}")
            progress.status = TrainingStatus.FAILED
            raise TrainingException(f"–û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {str(e)}")
        
        finally:
            if training_id in self.active_trainings:
                del self.active_trainings[training_id]
    
    async def fine_tune_model(
        self,
        model_name: str,
        model_version: Optional[str],
        training_examples: List[Dict[str, Any]],
        epochs: int = 10,
        batch_size: int = 16,
        learning_rate: float = 0.0001,
        freeze_embedding: bool = True
    ) -> Dict[str, Any]:
        """
        –î–æ–æ–±—É—á–µ–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –º–æ–¥–µ–ª–∏
        
        Args:
            model_name: –ò–º—è –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
            model_version: –í–µ—Ä—Å–∏—è –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
            training_examples: –ù–æ–≤—ã–µ –ø—Ä–∏–º–µ—Ä—ã
            epochs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
            batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
            learning_rate: Learning rate (–æ–±—ã—á–Ω–æ –º–µ–Ω—å—à–µ, —á–µ–º –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ —Å –Ω—É–ª—è)
            freeze_embedding: –ó–∞–º–æ—Ä–æ–∑–∏—Ç—å —Å–ª–æ–π —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –¥–æ–æ–±—É—á–µ–Ω–∏—è
        """
        training_id = str(uuid.uuid4())
        start_time = datetime.utcnow()
        
        logger.info(f"üîÑ –ù–∞—á–∞–ª–æ –¥–æ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: {model_name}")
        
        try:
            # –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
            model, vocab, encoders = self.model_manager.load_model(
                model_name, model_version
            )
            
            # –ó–∞–º–æ—Ä–æ–∑–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –µ—Å–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è
            if freeze_embedding:
                for param in model.embedding.parameters():
                    param.requires_grad = False
                logger.info("üîí –°–ª–æ–π —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∑–∞–º–æ—Ä–æ–∂–µ–Ω")
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            dataset = self._prepare_dataset(training_examples, vocab, encoders)
            train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
            
            # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ –∫—Ä–∏—Ç–µ—Ä–∏–π
            optimizer = optim.Adam(
                filter(lambda p: p.requires_grad, model.parameters()),
                lr=learning_rate
            )
            criterion = nn.CrossEntropyLoss()
            
            # –î–æ–æ–±—É—á–µ–Ω–∏–µ
            model.train()
            best_loss = float('inf')
            
            for epoch in range(epochs):
                epoch_loss = 0.0
                
                for batch in train_loader:
                    text = batch['text'].to(self.device)
                    status = batch['status'].to(self.device)
                    
                    optimizer.zero_grad()
                    outputs = model(text)
                    loss = criterion(outputs, status)
                    loss.backward()
                    optimizer.step()
                    
                    epoch_loss += loss.item()
                
                avg_loss = epoch_loss / len(train_loader)
                best_loss = min(best_loss, avg_loss)
                
                logger.info(f"Epoch {epoch + 1}/{epochs} | Loss: {avg_loss:.4f}")
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–æ–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
            new_model_name = f"{model_name}_finetuned"
            version = self.model_manager.save_model(
                model=model,
                vocab=vocab,
                encoders=encoders,
                model_name=new_model_name,
                metadata={
                    "training_id": training_id,
                    "base_model": model_name,
                    "base_version": model_version,
                    "fine_tuned": True,
                    "epochs": epochs,
                    "new_examples": len(training_examples),
                    "best_loss": best_loss
                }
            )
            
            duration = (datetime.utcnow() - start_time).total_seconds()
            
            return {
                "training_id": training_id,
                "status": "completed",
                "model_name": new_model_name,
                "model_version": version,
                "base_model": model_name,
                "new_examples": len(training_examples),
                "epochs_completed": epochs,
                "final_loss": best_loss,
                "duration_seconds": int(duration)
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–æ–±—É—á–µ–Ω–∏–∏: {e}")
            raise TrainingException(f"–û—à–∏–±–∫–∞ –¥–æ–æ–±—É—á–µ–Ω–∏—è: {str(e)}")
    
    def get_training_progress(self, training_id: str) -> Optional[TrainingProgress]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è"""
        return self.active_trainings.get(training_id)
    
    def _prepare_encoders(self, training_examples: List[Dict]) -> Dict:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —ç–Ω–∫–æ–¥–µ—Ä–æ–≤"""
        status_encoder = LabelEncoder()
        statuses = [ex['labels']['status'] for ex in training_examples]
        status_encoder.fit(statuses)
        return {'status': status_encoder}
    
    def _prepare_dataset(
        self,
        training_examples: List[Dict],
        vocab: Vocabulary,
        encoders: Dict
    ) -> TaskDataset:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        texts = [ex['text'] for ex in training_examples]
        vocab.build_from_texts(texts)
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ –Ω—É–∂–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
        from app.core.dataset import TrainingData, TaskInfo
        
        formatted_examples = []
        for ex in training_examples:
            labels = ex['labels']
            task_info = TaskInfo(
                name=labels['name'],
                description=labels['description'],
                priority=labels['priority'],
                deadline=labels.get('deadline'),
                execution_time=labels['execution_time'],
                category=labels['category'],
                difficulty=labels['difficulty'],
                stages=labels['stages'],
                status=labels['status']
            )
            formatted_examples.append(TrainingData(text=ex['text'], labels=task_info))
        
        return TaskDataset(
            texts=[ex.text for ex in formatted_examples],
            labels=[ex.labels for ex in formatted_examples],
            vocab=vocab,
            encoders=encoders
        )
