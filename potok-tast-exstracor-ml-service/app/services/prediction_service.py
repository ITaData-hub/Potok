import torch
from typing import List, Optional, Dict, Any
from datetime import datetime

from app.config.settings import get_settings
from app.utils.logger import setup_logger
from app.utils.exceptions import ModelNotLoadedException, PredictionException
from app.services.model_manager import ModelManager
from app.core.rules_engine import ParsingRulesEngine
from app.schemas.task import TaskResponse
from dataclasses import dataclass

settings = get_settings()
logger = setup_logger("prediction_service", settings.LOG_LEVEL)

@dataclass
class TaskInfo:
    """–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞"""
    name: str
    description: str
    priority: int
    deadline: Optional[str]
    execution_time: str
    category: List[str]
    difficulty: int
    stages: List[str]
    status: str
    confidence: float = 0.0

class PredictionService:
    """–°–µ—Ä–≤–∏—Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    
    def __init__(self):
        self.model_manager = ModelManager()
        self.model = None
        self.vocab = None
        self.encoders = None
        self.rules_engine = ParsingRulesEngine()
        self._cache: Dict[int, TaskInfo] = {}
        self.metrics = {
            'predictions': 0,
            'cache_hits': 0,
            'errors': 0
        }
        self.device = settings.DEVICE
    
    def load_model(
        self,
        model_name: Optional[str] = None,
        version: Optional[str] = None
    ):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π"""
        if model_name is None:
            model_name = settings.MODEL_NAME
        
        try:
            self.model, self.vocab, self.encoders = self.model_manager.load_model(
                model_name=model_name,
                version=version,
                device=self.device
            )
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {model_name}")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            raise ModelNotLoadedException(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å: {e}")
    
    def predict(self, text: str) -> TaskResponse:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –æ–¥–Ω–æ–π –∑–∞–¥–∞—á–∏
        
        Args:
            text: –¢–µ–∫—Å—Ç –∑–∞–¥–∞—á–∏
            
        Returns:
            –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∑–∞–¥–∞—á–µ
        """
        if self.model is None:
            raise ModelNotLoadedException("–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–µ—à–∞
        cache_key = hash(text)
        if cache_key in self._cache:
            self.metrics['cache_hits'] += 1
            cached_result = self._cache[cache_key]
            return self._convert_to_response(cached_result)
        
        self.metrics['predictions'] += 1
        
        try:
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –ø–æ–º–æ—â—å—é –ø—Ä–∞–≤–∏–ª
            task_features = self._extract_features_from_rules(text)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç—å—é
            self.model.eval()
            with torch.no_grad():
                encoded_text = torch.tensor(
                    self.vocab.encode(text),
                    dtype=torch.long
                ).unsqueeze(0).to(self.device)
                
                outputs = self.model(encoded_text)
                probabilities = torch.softmax(outputs, dim=1)
                status_idx = outputs.argmax(dim=1).item()
                confidence = probabilities[0, status_idx].item()
                status = self.encoders['status'].decode(status_idx)
            
            # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            result = TaskInfo(
                name=task_features['name'],
                description=task_features['description'],
                priority=task_features['priority'],
                deadline=task_features['deadline'],
                execution_time=task_features['execution_time'],
                category=task_features['category'],
                difficulty=task_features['difficulty'],
                stages=task_features['stages'],
                status=status,
                confidence=confidence
            )
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫–µ—à
            self._cache[cache_key] = result
            
            return self._convert_to_response(result)
            
        except Exception as e:
            self.metrics['errors'] += 1
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {e}")
            raise PredictionException(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏: {str(e)}")
    
    def predict_batch(self, texts: List[str]) -> List[TaskResponse]:
        """
        –ü–∞–∫–µ—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        
        Args:
            texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –∑–∞–¥–∞—á
            
        Returns:
            –°–ø–∏—Å–æ–∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        """
        results = []
        for text in texts:
            try:
                result = self.predict(text)
                results.append(result)
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –≤ –ø–∞–∫–µ—Ç–Ω–æ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏: {e}")
                # –ú–æ–∂–Ω–æ –ª–∏–±–æ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å, –ª–∏–±–æ –¥–æ–±–∞–≤–∏—Ç—å –∑–∞–≥–ª—É—à–∫—É
                continue
        
        return results
    
    def _extract_features_from_rules(self, text: str) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –ø–æ–º–æ—â—å—é rule-based –ø–æ–¥—Ö–æ–¥–∞"""
        return {
            'name': self.rules_engine.extract_title(text),
            'description': self.rules_engine.extract_description(text),
            'priority': self.rules_engine.extract_priority(text),
            'deadline': self.rules_engine.extract_deadline(text),
            'execution_time': self.rules_engine.extract_time(text),
            'category': self.rules_engine.extract_category(text),
            'difficulty': self.rules_engine.extract_complexity(text),
            'stages': self.rules_engine.extract_stages(text)
        }
    
    def _convert_to_response(self, task_info: TaskInfo) -> TaskResponse:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –≤ API response"""
        return TaskResponse(
            name=task_info.name,
            description=task_info.description,
            priority=task_info.priority,
            deadline=task_info.deadline,
            execution_time=task_info.execution_time,
            category=task_info.category,
            difficulty=task_info.difficulty,
            stages=task_info.stages,
            status=task_info.status,
            confidence=task_info.confidence,
            processed_at=datetime.utcnow()
        )
    
    def get_metrics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ —Å–µ—Ä–≤–∏—Å–∞"""
        return {
            **self.metrics,
            'cache_size': len(self._cache),
            'vocab_size': self.vocab.vocab_size if self.vocab else 0,
            'model_loaded': self.model is not None
        }
    
    def clear_cache(self) -> int:
        """–û—á–∏—Å—Ç–∫–∞ –∫–µ—à–∞"""
        cache_size = len(self._cache)
        self._cache.clear()
        logger.info(f"üßπ –ö–µ—à –æ—á–∏—â–µ–Ω: {cache_size} –∑–∞–ø–∏—Å–µ–π")
        return cache_size
