"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –∏–∑ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
"""
import asyncio
import json
import sys
from pathlib import Path

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫–æ—Ä–Ω–µ–≤–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –≤ path
sys.path.insert(0, str(Path(__file__).parent.parent))

from app.services.training_service import TrainingService
from app.utils.logger import setup_logger

logger = setup_logger("train_script", "INFO")

async def train_from_file(
    data_file: str,
    epochs: int = 30,
    batch_size: int = 32,
    learning_rate: float = 0.001,
    model_name: str = "task_extraction_model"
):
    """
    –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏–∑ JSON —Ñ–∞–π–ª–∞
    
    Args:
        data_file: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        epochs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
        batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
        learning_rate: Learning rate
        model_name: –ò–º—è –º–æ–¥–µ–ª–∏
    """
    logger.info(f"üìö –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {data_file}")
    
    try:
        with open(data_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        training_examples = data.get('training_examples', [])
        
        if len(training_examples) < 10:
            logger.error("‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö (–º–∏–Ω–∏–º—É–º 10 –ø—Ä–∏–º–µ—Ä–æ–≤)")
            return
        
        logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(training_examples)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Å–µ—Ä–≤–∏—Å–∞ –æ–±—É—á–µ–Ω–∏—è
        training_service = TrainingService()
        
        # –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
        logger.info("üöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è...")
        result = await training_service.train_new_model(
            training_examples=training_examples,
            epochs=epochs,
            batch_size=batch_size,
            learning_rate=learning_rate,
            model_name=model_name,
            save_checkpoint=True
        )
        
        logger.info("=" * 70)
        logger.info("‚úÖ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û")
        logger.info("=" * 70)
        logger.info(f"–ú–æ–¥–µ–ª—å: {result['model_name']}")
        logger.info(f"–í–µ—Ä—Å–∏—è: {result['model_version']}")
        logger.info(f"–ü—Ä–∏–º–µ—Ä–æ–≤: {result['total_examples']}")
        logger.info(f"–≠–ø–æ—Ö: {result['epochs_completed']}")
        logger.info(f"–§–∏–Ω–∞–ª—å–Ω—ã–π loss: {result['final_loss']:.4f}")
        logger.info(f"–í—Ä–µ–º—è: {result['duration_seconds']} —Å–µ–∫")
        logger.info("=" * 70)
        
    except FileNotFoundError:
        logger.error(f"‚ùå –§–∞–π–ª {data_file} –Ω–µ –Ω–∞–π–¥–µ–Ω")
    except json.JSONDecodeError:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON –≤ {data_file}")
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞: {e}", exc_info=True)

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∑–∞–¥–∞—á')
    parser.add_argument('data_file', type=str, help='–ü—É—Ç—å –∫ JSON —Ñ–∞–π–ª—É —Å –¥–∞–Ω–Ω—ã–º–∏')
    parser.add_argument('--epochs', type=int, default=30, help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö')
    parser.add_argument('--batch-size', type=int, default=32, help='–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞')
    parser.add_argument('--lr', type=float, default=0.001, help='Learning rate')
    parser.add_argument('--name', type=str, default='task_extraction_model', help='–ò–º—è –º–æ–¥–µ–ª–∏')
    
    args = parser.parse_args()
    
    asyncio.run(train_from_file(
        data_file=args.data_file,
        epochs=args.epochs,
        batch_size=args.batch_size,
        learning_rate=args.lr,
        model_name=args.name
    ))
